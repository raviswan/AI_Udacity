AI NOTES

Minimax search and Alpha-Beta Pruning: https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm

DFS, BFS: https://mhesham.wordpress.com/tag/iterative-deepening-depth-first-search/

------- Submitting project -------

source activate aind
source deactivate aind
udacity submit <projname>


- AI terms: Roomba example. Agent, State, Action, Cognition.

-------------------------------------------------------------------------
		Min-Max / ISOLATION-GAME 
-------------------------------------------------------------------------

- Remember the game where we , player has to be the last to make a move. It's also called Isolation game.

- Your goal is to maximize number of moves. Your opponents goal is to minimize number of moves. For this we gotta , do bottoms-up. Look at the terminating condition. If you win, count it as +1, if opponent wins, make it -1. These values can be anything but the bigger the range between them, better the outcome.

- At your level, your job is to maximize the outcome. Therefore, from your child branches, you pick the max. value of all child branches (for our example +1). At min. level, your opponent would do the opposite. That is pick a minimum value of -1. This percolates all the way to the top to let you visualize which is the best move that yields +1. 

- As the board size gets bigger, the number of combinations to consider at the each level gets exponentially bigger. So we gotta come up with better ways to play in the limited time we get. One way is to limit the number of branches you consider at each level. For e.g at the start of 5x5 game, the "DEPTH", d= 25.

- If you start, you have 25 places to choose from, and your opponent has 24 places to choose from. After that , you don't really have 23 sub-branches to consider for your next move. Some of them may be blocked off. When you run average, you determine that only 9 branches on an average are required per move. Think logically, by the time you get towards the end of the game, you'd have 4 moves to make, you oppoent would have 3, you 2, your opponent 1 and then you lose.
Now, if we assume we can look athe only 8 branches (called "BRANCHING FACTOR" b) at each level, which branch would you pick. You need a metric for that.   It is established that number of "nodes" minimax" will need to visit is (b^d). In this example, that would be 8^25. This is where you need a heuristic function to compute the metrics . A simple one would be #my_moves. So at the every level, you compute number of remaining moves for you.

If you have a multiplayer isolation game, you form a set of of (3 metrics for 3 player game at each level). [a,b,c]. At the Level 3, you pick the set that maximizes `c` , and propogate it upward. At level 2,
you pick the set that maximizes 'b'. And at level 1, you pick the set that maximizes 'a'.

Now if you have probability of how often a position is likely to be picked , then it's best to have a range for maximum value each player metric can take . For e.g, if the range is [-10:10], and lets say at a given level, the probability of picking a move is 0.9, and the max metric at the level below that is 10. Then, at current level, you'd have 0.9*10 = 9 . You can denote this as >=9. Now compare this with the peers and you can do what is called Alpha-beta pruning. 



--------DEPTH LIMITED SEARCH ------
In this, you basically go upto a fixed depth and return the metrics from there instead of waiting to hit terminal condition. 
Quiescence is when your branch that is pickec starts to remain the same as you go deeper and deeper.

-------ITERATIVE FIRST DEEPENING SEARCH-----

http://www.geeksforgeeks.org/iterative-deepening-searchids-iterative-deepening-depth-first-searchiddfs/

In iterative deepening, for every move, you run alphabeta function for all depths starting from 0 until until you run out of time. 

----- ALPHA BETA Pruning -----
Essentially at every minmax level, you compute the local min or local max, starting from left-most branch. And use that
to prune other moves for the same "for" loop.  

For instance, in max(),  you adjust the alpha for max computation to essentially find values >= alpha for subsequent "for" loop entries of the max(). And this alpha is passed down to the min() one level down  as part of the next index of  "for" loop of the current max (). And if that entry in min() returns a value that is less than alpha we passed in, don't do further computation in min(). Just return. So you pruned the tree.

In min(),  you adjust the beta for min computation to essentially find values <= beta for subsequent "for" loop entries of the min (). And this beta is passed down to the max() one level down as part of the next index of "for" loop of the current min(). And if that entry in the max() returns a value that is greater than beta we passed in, don't do further computation in max(). Just return. So you pruned the tree.


-------------------------------------------------------------------------
		SEARCH ALGORITHMS 
-------------------------------------------------------------------------

BFS, CFS, DFS, Uniform Cost, Greedy Best-first search, A*:

Remember: Frontier set, Explored set and UnExplored set. Remember, going from Arad to Romania

BFS, CFS: are complete, optimal but require 2^n storage for operation
DFS: non-optimal, non-complete but requires only n storage space.


BFS or Shortest Path first
-------------------------- 
Shortest paths are added first. For e.g 1 hop away followed by 2 hops away etc.
When you pick one option (break a tie if there's one) from 1 hop payh, you then add the selected point's neighbors to the list. These new neighbors cost would be now 2 hops. So, after removing the visited notes, you go back
to the remaining 1 hop path. And work your way through until you find the destination.

The new paths are added to the end of the path list. Might need to store 2^n.

Cheapest Cost First
--------------------
always picks the cheapest path. After new paths are added, it still picks the shortest even if it means going back many levels.. So it goes back and forth. Optimal as well.

Depth First Search
 Can go on and on. Not optimal. But advantage is very few past states required to store. Of order of n if you don't have to store the explored set.

Uniform Cost search
-------------------
Remember spanning out in concentric circles till you hit goal.

Greedy Best-first Search
---------------------- 
is more directed. Instead of spanning out in all directions, you start with a constraint : lets say, with an estimate of  st. line distance between starting point and goal. This would mean you'd only move forward in the direction of direct as-the-crow-flies path between start and destination.
Now you keep going in that direction alone, until you find your target. Howeverm if you encounter an obstacle in the st. line path, this algo would fail in finding the best path esp when the option would have been to deviate from the path to find the best path.

A* search
----------
combines the best of Uniform Cost and Greedy best first search. Look at it as a combo of bfs with heuristic.

goal is to minimize function 
f = g + h
where g - path cost as before
	  h - heuristic to estimate cost to reach goal from the state with cost g. For destination, it can be st. line distance from intermediate point to the goal. Remember g is path cost, which may be winding roads, so need to be straight line, whereas h is estimate of st. line distance. Again go back to Romania example

You always pick the minimum f as you continue exploring to find the cheapest path, shortest path.

As long cost of heuristic h is < true cost, the algo  will always return the lowest cost path. In this case,
h is called optimistic, admissible.

How you come up with h is the where all the intelligence is. 
In general, all admissible h you come up with, max(h1,h2, h3) is best used as that would result in fewest searches
to the goal.

Simulated Annealing:
========================
You are tasked with finding highest point in  2 dimensional mountain range. You can do random start and and go either side and keep track of the maximum.
Else you pick a random point, if the step size to either right or left is increasing , keep going.
If the step size results in decreasing height, then do a random swing to determine the next point to measure.
This random swing is chosen with probability of  e^(Expected value delta/ T) . Initially start with high  like inifnity.
Now, we get e^0 = 1. As T gets smaller and smaller, the probability of pick gets smaller and smaller. When T reaches zero, we have our target answer.



Backtracking search and Constraint Propagation
============================================

For Australia painting example:

Least Constraining Value:
Take the example of colorin Australia map using 3 colors. You can start from left. At some point, you have to decide which color to use on which region. If you pick a color for current region that facilitates making future choice  it is called Least Constraining Value.

Minimum Remaining Value:
Another strategy for backtracking search  is to pick the region that has least choices to pick from. 
Refer to "Constraint Satisfaction" section of Udacity AI

Forward Checking:

Start with a palette of all colors for all regions. Pick one color. Remove these colors from palette of neighbors.
Keep doing this until you run out of colors for one region. At which point you backtrack.


Arc Consistent:
This means there's no region which runs out of colors.

Propositional Logic:
======================
Same as Boolean logic. But remember,  truth tables have slight different meaning for these two operations
P => Q (P implies Q) and
P <=> Q (equivalent)
 When P is true, and Q is false, the outcome is false.

Propositional logic is when the variables can be in either True, False 
In probablility, the variables get a value.

What is a model?
Model is a value for each propositional symbol e.g. Model is {P: True, Q: False}

If all conditions in a model are satisfied, the model or sentence is "valid"
if some conditons are satisfied, the model is "satisfiable"
If all conditions are false, the model is "unsatisfiable"

 First Order Logic:
====================
Made up for variables, functions, relations, quantifiers to model real world scenario. Represents objects. 
Two quantifiers are:  "For all"(For all x ...) given by symbol A, and  "There exists" (There exists an x ...) given by inverted A.

Remember ^ is AND, v is OR and there's NOT. This is propositional logic.

When the action is non-deterministic, you start introducing Loop() which would loop
if the action doesn't cause change in state. Remember, the " move right" operation of vacuum cleaner won't always succeeed if the vaccuum wheel slips. You have to represent that part as loop.


Action Schema
==================

Remember the problem of shipping cargo from one to airport to another. This action can be done for 100's of cargo 
 flying from any of the 100's airports. The states can be generically represented as:

 Action (Fly(p,from,to))
    PRECOND: Plane(p) ^  Aiport(from) ^ Airport(to) ^ At (p,from)
    EFFECT: Not(At(p,from)) ^ At(p, to)

Action (Load(c,p,a))
    PRECOND: At(c,a) ^ At(p,a) ^ Cargo(c) ^ Plane(p) ^ Airport(a)
    EFFECT: Not(At (c,a)) and In(c,p) 

Action (Unload(c,p,a))
    PRECOND: In(c, p) ^ At(p,a) ^ Cargo(c) ^ Plane(p) ^ Airport(a)
    EFFECT:   At(c,a)) and Not(In(c,p)) 



Here every action would result in a new state. 
For e.g. when cargo C, is loaded onto plane at SFO. This would result in a state of cargo loaded on plane P.
We keep moving forward until we reach a goal. This is called  "Progression State Space search".

The opposite of this approach is "Regression State Space search " where we start from Goal State and search back.


FIRST ORDER LOGIC FOR PLANNING
===============================

Actions: objects E.g.   Fly(p,from,to) is an object
Predicates that can vary from one state to another are called Fluents.
Predicates:  like plane is at airport x (i.e. )At(p,x) are called Fluents 

For cargo shipping problem across airports, you define initial state as bunch of positive fluents
and negative fluents.  Both fluents put together make Initial State.
On this initial state, you take some action
https://towardsdatascience.com/ai-planning-historical-developments-edcd9f24c991

Planning Graph
===============
Way to solve determinstic problems that has exponential space, but planning graph
reduces it to polynomial space. Planning graph is made up of levels and actions.
And the problem is solved until we reach a point where two levels become identical.
ie. Action on one level doesn't change the fliuents. That indicates to us that we have arrived
at the solution.

The problem starts with Initial state of Level0 followed by Action State A0.
A0 on Level 0 fluents lead to Level 1. A1 on Level1 fluent leads to Level 2 etc.
Refer to Eat Cake example.

To determine a heuristic, you can do:

1) Ignore predconditions: which treats every action applicable in every state. No preconditions,
so at every level, number of goals unsatisfied is the heuristic. Keep summing it up unitl the problem is solved
2) Level-Sum: This heuristic is obtained by counting the level cost of goal fluents.
You find the level at which a goal is first satisfied. That becomes the level cost for that goal.
Likewise, do it for all goals and sum all the level costs.

=============
ASL RECOGNIZER:
=============
Here every video is split into frames. From every frame, we note the x,y values of
nose and right and left hand. These values are stored in a table.
From these values, we get the metric called features. E.g.
leftx,rightx, lefty, right-y, nose-x, nosey. From these basic, we can
get many different feature set as follows:
1)features_ground,
2)features_polar
3)features_delta
4)features_diff

Next, we have a training set. This consists of a file with frame data for a number of
commonly occuring words.
Each word will be contained in multiple frames with start-frame and end-frame. And this would
be repeated many times.
What you do is group these words in a dataframedf) using (pandas) and store it in dictionary.
For e.g. from the file, we process dict in this form like :

{   "JOHN": [ ([occurence 1start frame feature] ...[end-frame feature]
            [occurnece 2 start framefeature] ...[end-frame feature]
            [occurence 3 start framefeature] ...[end-frame feature]), 
                [
                Total frame Length of 1st occurence, 
                Total Frame length of second occurence, 
                Total Frame length of third occurence
                ]
           ]
    "MARY": [ [occurence 1start frame feature] ...[end-frame feature]
                [occurnece 2 start framefeature] ...[end-frame feature]
                [occurence 3 start framefeature] ...[end-frame feature]
               ]
                [
                Total frame Length of 1st occurence, 
                Total Frame length of second occurence, 
                Total Frame length of third occurence
                ]
            ]

}

Once you have the training set, the next step is to find the number of hidden states in HMM
using this data. 
To do this, you have to pass the training data through different ModelSelectors  (BIC, DIC 
or CrossValidation Folds).

Each word is based tried different states (from 2 to 15) and the one with max. likelihood value is predicted.


What's the significance of Gaussian distribution here. It implies that every observable for every hidden 
state is modeled as a gaussian distribution. remember the weather , ice-cream example. Instead of 
giving 1,2,3 ice-creams, we are gonna have a Gaussian distributed ice-cream values for every state 
in the HMM.

Each word will have its own Gaussian HMM model object that is returned by the Model Selectors below.

BIC: It penalizes longer models. Has two terms, The lower the value, the better it is
====
DIC: 
====
It wants to find a state count that gives high logL for current value while at the same time minimizng 
logL count for other words.

Crossvalidation Folds(CV)
=========================
Here , training data is split into folds. In each fold, one part of the data
is used to train the model, which is then run on the test part . The score from each  fold is summed and eventually
 averaged across all folds
 This average is computed for model state count (n=2 to 15), and the state count that has the max. average
 is picked to be the best model. This process is called ModelSelector.


And once the operation is complete, we have a dict of words as :
{word: BestHMMModel object}


Last step is to guess the words:
Here we take the test data (stored in rows - one for each word)
Take the frame data for each word (i.e. each row) and run it through the 
HMM dictionary. That is, you determine model corresponding to which word outputs the highest logL value
for the given row of test data. Store that word as the best guess. 
Do this for the words in the test data. And at the end of it , count the % of correct guesses.


This project used Jupyter and Panda
